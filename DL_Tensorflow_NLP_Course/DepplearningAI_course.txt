W1 - Sentiment in text
Tokenization
New Headline Sarcasm
BBC News Headline
Introduction, A conversation with Andrew Ng
Introduction
"Word based encodings"
Using APIs
Notebook for lesson 1
Text to sequence
Looking more at the "Tokenizer"
"Padding"
Notebook for lesson 2
Sarcasm, really?
Working with the Tokenizer
@News headlines dataset for sarcasm detection
Notebook for lesson 3
Week 1 Wrap up
LTI Item: Exercise 1- Explore the BBC news archive
@LTI Item Exercise  Answer Explore the BBC news archive

Word Embedding
Tokenizing
Text to Padded Sequences using Word Embedding-(Tokenizing)
///////////////////////////////////Tokenizer////////////////////////////////////////////
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#sentences =  List of sentences 
tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>") # num_words = n most common words from the text # oov_token = Out Of Vocab
tokenizer.fit_on_texts(sentences)

#{word:index}
word_index = tokenizer.word_index
#{'<OOV>': 1, 'apple' = 2, ....}

#Sequences
sequences = tokenizer.texts_to_sequences(sentences)
#[[4,5,2,1],[7,5,2],[4,2],[9,4,5,8,10,9]....]

#padding
padded = pad_sequences(sequences, maxlen=5, padding='post') # maxlen = By Default Longest Sentence  # padding = by default 'pre'
#[[0,0,0,0,0,4,5,2,1],[0,0,0,0,0,0,7,5,2],[0,0,0,0,0,0,0,4,2],[0,0,0,0,9,4,5,8,10,9]....]


///////////////////////////////////Tokenizer////////////////////////////////////////////

///////////////////////////////////REMOVE STOPWORDS////////////////////////////////////////////
from gensim.parsing.preprocessing import remove_stopwords

# pass the sentence in the remove_stopwords function
filtered_sentences=[]
for text in sentences:
    result = remove_stopwords(text)
    filtered_sentences.append(result)
///////////////////////////////////REMOVE STOPWORDS////////////////////////////////////////////

Data Set
BBC News Text https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv
	category		text
0	tech			tv future in the hands of viewers with home th...
1	business		worldcom boss left books alone former worldc...
2	sport			tigers wary of farrell gamble leicester say ...
3	sport			yeading face newcastle in fa cup premiership s...
4	entertainment	ocean s twelve raids box office ocean s twelve...
Category  = {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}





--------------------------------------------------------------------------------------------------------------------
W2 - Word Embeddings
IMDB Dataset
	http://ai.stanford.edu/~amaas/data/sentiment/
	You will find here 50,000 movie reviews which are classified as positive of negative.

To Install datasets in tensrflow
!pip intall -q tensorflow-datasets


import tensorflow_datasets as tfds
imdb, info = tfds.load("imdb_reviews",with_info=True, as_supervised = True)



Tensorflow Datasets
https://www.tensorflow.org/datasets/catalog/overview



Embedding layer gives the vector value of words?
Correct. If embedding_dim = 64, every word is mapped to a vector in a 64-dimensional space (a vector with 64 elements).


- This week you looked at taking your tokenized words and using "EMBEDDINGS" to establish meaning from them in a mathematical way. 
- Words were mapped to vectors in higher dimensional space, and the semantics of the words then learned when those words were labelled with similar meaning. 
- So, For example, when looking at movie reviews, those movies with positive sentiment had the dimensionality of their words ending up ‘pointing’ a particular way, and those with negative sentiment pointing in a different direction. 
- From this, the words in future sentences could have their ‘direction’ established, and from this the sentiment inferred. 
- You then looked at "SUB WORD TOKENIZATION", and saw that not only do the meanings of the words matter, but also the sequence in which they are found.



vocab_size = 1000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'
training_portion = .8


tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index

train_sequences = tokenizer.texts_to_sequences(train_sentences)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)
(1780,120)

validation_sequences = tokenizer.texts_to_sequences(validation_sentences)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)
(445, 120)


label_tokenizer = Tokenizer()
label_tokenizer.fit_on_texts(labels)

training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))
(1780, 1)
validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))
(445, 1)


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
num_epochs = 30
history = model.fit(train_padded,training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)
>>>
	Epoch 30/30
	1780/1780 - 0s - loss: 0.0376 - acc: 0.9983 - val_loss: 0.1951 - val_acc: 0.9461

model.summary()
>>>
	Model: "sequential"
	_________________________________________________________________
	Layer (type)                 Output Shape              Param #   
	=================================================================
	embedding (Embedding)        (None, 120, 16)           16000     
	_________________________________________________________________
	global_average_pooling1d (Gl (None, 16)                0         
	_________________________________________________________________
	dense (Dense)                (None, 24)                408       
	_________________________________________________________________
	dense_1 (Dense)              (None, 6)                 150       
	=================================================================
	Total params: 16,558
	Trainable params: 16,558
	Non-trainable params: 0



reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
(27285)

e = model.layers[0]
weights = e.get_weights()[0]
(1000, 16)

import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()



/////////////////////////////////REMOVING STOP WORDS USING NLTK////////////////////////////////////////////
#REMOVING STOP WORDS USING NLTK
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

from nltk.tokenize import word_tokenize 
nltk.download('punkt')

corpus_ = ['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
			'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.']

filtered_courpus=[]
for sentence in corpus_:
	word_tokens = word_tokenize(sentence)
	filtered_sentence = [] 
	for w in word_tokens: 
	    if w not in stop_words: 
	        filtered_sentence.append(w) 
	filtered_courpus.append(" ".join(filtered_sentence))
corpus_,filtered_courpus
len(corpus_[0]),len(filtered_courpus[0])
"""
(['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
  'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.'],
 ['He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts .',
  'He ready becuase rights become much less valuable , indeed vaguest idea wood river question .'],
 128,
 87)
 """
///////////////////////////////////REMOVING STOP WORDS USING NLTK//////////////////////////////////////////

///////////////////////////////////REMOVING STOP WORDS USING SPACY//////////////////////////////////////////

#  "nlp" Object is used to create documents with linguistic annotations.
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English

# Load English tokenizer, tagger, parser, NER and word vectors
nlp = English()

corpus_ = ['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
			'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.']

filtered_corpus=[]
for text in corpus_:
  my_doc = nlp(text)
  token_list = [token.text for token in my_doc]
  filtered_sentence =[] 
  for word in token_list:
    lexeme = nlp.vocab[word]
    if lexeme.is_stop == False:
      filtered_sentence.append(word)   
  filtered_corpus.append(" ".join(filtered_sentence))
corpus_,filtered_courpus,len(corpus_[0]),len(filtered_courpus[0])
"""
(['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
  'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.'],
 ['determined drop litigation monastry , relinguish claims wood - cuting fishery rihgts .',
  'ready becuase rights valuable ,   vaguest idea wood river question .'],
 128,
 86)
 """
///////////////////////////////////REMOVING STOP WORDS USING SPACY//////////////////////////////////////////

///////////////////////////////////REMOVING STOP WORDS USING Genism//////////////////////////////////////////

from gensim.parsing.preprocessing import remove_stopwords
corpus_ = ['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
			'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.']
# pass the sentence in the remove_stopwords function
filtered_courpus=[]
for text in corpus_:
    result = remove_stopwords(text)
    filtered_courpus.append(result)
corpus_,filtered_courpus,len(corpus_[0]),len(filtered_courpus[0])

"""
(['He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.',
  'He was the more ready to do this becuase the rights had become much less valuable, and he had  indeed the vaguest idea where the wood and river in question were.'],
 ['He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once.',
  'He ready becuase rights valuable, vaguest idea wood river question were.'],
 128,
 90)
 """
///////////////////////////////////REMOVING STOP WORDS USING Genism//////////////////////////////////////////


--------------------------------------------------------------------------------------------------------------------
W3 - Sequence models
In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. 
But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'. 
This week you`ll start digging into a variety of model formats that are used in training models to understand CONTEXT IN SEQUENCE!

"Text Generation"




You’ve been experimenting with NLP for text classification over the last few weeks. Next week you’ll switch gears -- and take a look at using the tools that you’ve learned to predict text, which ultimately means you can create text. 
By learning sequences of words you can predict the most common word that comes next in the sequence, and thus, when starting from a new sequence of words you can create a model that builds on them. 
You’ll take different training sets -- like traditional Irish songs, or Shakespeare poetry, and learn how to create new sets of words using their embeddings!


Single Layer BiDirectional LSTM
Multiple Layer (Stacked) BiDirectional LSTM
Conv2d 






--------------------------------------------------------------------------------------------------------------------
W4 - Sequence models and literature


seed_text = "Laurence went to dublin"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_text)
"Laurence went to dublin" a pound would fall lanigan eyes academy academy academy academy water steps stepped out in lanigans ball ball ball lanigans ball dublin dublin dublin dublin steps relations relations relations relations relations relations relations relations ladies relations glisten a lanigan mchugh your kerrigan glisten glisten glisten water steps stepped out in lanigans ball couples and ladies ladies was painted painted painted painted painted runctions kinds nonsensical polkas mavrone taras hall hall hall glisten glisten water academy academy water steps she stepped out and she stepped in again again again again red academy steps a hall hall brothers entangled entangled academy entangled



seed_text = "I've got a bad feeling about this"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_text)
"I've got a bad feeling about this" half dozen stout died and laid the song of the warrior bard shaken silver home now since i spent up in dublin call belfast city and the brother william stood at the door and they ring at me diggin for erin go bragh together again soon i patrick up her pipes bellows chanters and all up with a glass of love easy as the sea came to james connolly cry my a going to a baby on free sweep but away me forget old ireland all care of me darling in strife roaming tomorrow i paid than him went by

https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt


Over the last four weeks you`ve gotten a grounding in how to do Natural Language processing with TensorFlow and Keras. You went from first principles -- basic Tokenization and Padding of text to produce data structures that could be used in a Neural Network.

You then learned about embeddings, and how words could be mapped to vectors, and words of similar semantics given vectors pointing in a similar direction, giving you a mathematical model for their meaning, which could then be fed into a deep neural network for classification.

From there you started learning about sequence models, and how they help deepen your understanding of sentiment in text by not just looking at words in isolation, but also how their meanings change when they qualify one another.

You wrapped up by taking everything you learned and using it to build a poetry generator!

This is just a beginning in using TensorFlow for natural language processing. I hope it was a good start for you, and you feel equipped to go to the next level!




seed_text = "Help me Obi Wan Kenobi, you're my only hope"
next_words = 100
  
for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
	predicted = model.predict_classes(token_list, verbose=0)
	output_word = ""
	for word, index in tokenizer.word_index.items():
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_text)


"Help me Obi Wan Kenobi, you're my only hope" are you with your rhyme rhyme part to tell pain ride go wide near survey rare exceeds razed gone or forth each ` lies go delight skill past way dwells quite fitted fitted twain hate men past days twain twain go rhyme delight tongue delight skill report light hits fitted sway`st quite fitted back live good rage doth date light quite quite had did give ill twain write `tis me life away things fire fire past youth so best live they forsworn thee die mine eyes grew ill decrease decrease in their moan me in hate night new new days






Have experience in projects like Digit Recognition, Image recognition, classification, object detection using skimage, PIL, CNN using tensorflow and keras. 
Working knowledge of OpenCV Framework.

Kindly refer to few of the projects done on Image Datasets.

1. Deep Neural Network – TensorFlow 
– Image Classification 
- Hand SIGNS dataset  
- Classify Hand gestures whether they symbolize 1,2,3,4 or Thumbs Up.

2. CNN – Keras – Image Classification
ConvNet in Keras for a classifying Emotion on Face whether Happy or Not

3. YOLO – Object Detection
- Images from car on road while driving
- Use object detection on a car detection dataset. Deal with bounding boxes. Use YOLO Pretrained model

4. RNN – LSTM – Sequence classification – Image
MNIST
If we consider every image row as a sequence of pixels, we can feed a LSTM network for classification.
https://colab.research.google.com/github/Gurubux/CognitiveClass-DL/blob/master/2_Deep_Learning_with_TensorFlow/DL_CC_2_3_RNN/3.4-Review-LSTM-MNIST-Database.ipynb#scrollTo=r5G9oTB1J0Sl


5. Action Recognition
Steps to Classify video clips based on the action performed in the Clip
https://github.com/Gurubux/Internships/blob/master/Quantiphi/Gurubux_Quantiphi_ProblemSet1_Sol.pdf